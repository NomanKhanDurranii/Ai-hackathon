{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876c320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b67b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Function to extract data for 4th structure from a page ---\n",
    "def extract_company_info(driver):\n",
    "    data = {}\n",
    "    \n",
    "    def safe_find_text(by, selector):\n",
    "        try:\n",
    "            return driver.find_element(by, selector).text.strip()\n",
    "        except NoSuchElementException:\n",
    "            return \"\"\n",
    "    \n",
    "    # Extract all key-value blocks inside the container\n",
    "    # Blocks with label and value classes\n",
    "    \n",
    "    blocks = driver.find_elements(By.CSS_SELECTOR, \"div.block-QCJM7wcY\")\n",
    "    for block in blocks:\n",
    "        try:\n",
    "            label = block.find_element(By.CSS_SELECTOR, \"div.label-QCJM7wcY\").text.strip()\n",
    "            # The value could be inside an <a> or <div> with class value-QCJM7wcY\n",
    "            try:\n",
    "                value = block.find_element(By.CSS_SELECTOR, \"a .value-QCJM7wcY\").text.strip()\n",
    "            except NoSuchElementException:\n",
    "                value = block.find_element(By.CSS_SELECTOR, \"div.value-QCJM7wcY\").text.strip()\n",
    "            \n",
    "            data[label] = value\n",
    "        except NoSuchElementException:\n",
    "            continue\n",
    "    \n",
    "    # Extract the about/description paragraph (last div with content-H16icEW0)\n",
    "    try:\n",
    "        description = driver.find_element(By.CSS_SELECTOR, \"div.container-H16icEW0 div.content-H16icEW0 span\").text.strip()\n",
    "        data['Description'] = description\n",
    "    except NoSuchElementException:\n",
    "        data['Description'] = \"\"\n",
    "    \n",
    "    return data\n",
    "\n",
    "# --- Main scraping logic ---\n",
    "def main():\n",
    "    # Chrome options to run headless (optional)\n",
    "    options = Options()\n",
    "    # options.add_argument(\"--headless\")  # Uncomment if you want no browser UI\n",
    "    \n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    # Read links from CSV file (symbol_link.csv)\n",
    "    links = []\n",
    "    with open(\"symbol_link.csv\", \"r\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        reader = csv.reader(file)\n",
    "        # Assuming the CSV has links in first column; skip header if any\n",
    "        for row in reader:\n",
    "            if row and \"http\" in row[0]:\n",
    "                links.append(row[0])\n",
    "\n",
    "    all_data = []\n",
    "    \n",
    "    for link in links:\n",
    "        print(f\"Processing: {link}\")\n",
    "        driver.get(link)\n",
    "        time.sleep(3)  # Wait for page load; adjust as needed\n",
    "\n",
    "        company_data = extract_company_info(driver)\n",
    "        company_data['URL'] = link  # Save source link for reference\n",
    "        all_data.append(company_data)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # Save all extracted data to CSV\n",
    "    df = pd.DataFrame(all_data)\n",
    "    df.to_csv(\"extracted_company_info.csv\", index=False, encoding=\"utf-8\")\n",
    "    print(\"Scraping done and data saved to extracted_company_info.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edf2ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_links_df = pd.read_csv(\"symbol_link.csv\")\n",
    "exchange_list = symbol_links_df[\"Exchange\"].tolist()\n",
    "link_list = symbol_links_df[\"Symbol\"].tolist()\n",
    "\n",
    "print(exchange_list)\n",
    "print(link_list)\n",
    "driver = webdriver.Chrome()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7fd13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sanitize_text(text):\n",
    "    # Remove special unicode characters and extra whitespace\n",
    "    cleaned = re.sub(r'[^\\x00-\\x7F]+', '', text)  # keep only ASCII\n",
    "    return cleaned.strip()\n",
    "company_info = []\n",
    "for i in range(len(link_list)):\n",
    "    url = f\"https://www.tradingview.com/symbols/{exchange_list[i]}-{link_list[i]}/\"\n",
    "    driver.get(url)\n",
    "    print(url)\n",
    "    time.sleep(3)\n",
    "\n",
    "    records = driver.find_elements(By.CSS_SELECTOR, \".wrapper-QCJM7wcY\")\n",
    "\n",
    "    \n",
    "    for record in records:\n",
    "        children = record.find_elements(By.CLASS_NAME, \"apply-overflow-tooltip\")\n",
    "\n",
    "        for j in range(min(23, len(children))):\n",
    "            raw_text = children[j].text\n",
    "            cleaned_text = sanitize_text(raw_text)\n",
    "            company_info.append(cleaned_text)\n",
    "\n",
    "  \n",
    "\n",
    "print(company_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
